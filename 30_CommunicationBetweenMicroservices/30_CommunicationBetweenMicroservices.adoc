= Inter microservices communication =

== Introduction ==
With microservices comes a set of new challenges. One of them is not only to think about the data getting transferred between your services and layer of applications, like with a monolith application, but also to think how this data is transferred. Indeed, it was relatively simple before. Since everything was running in a single application, all you had to do was a function call ! With microservices however, this becomes much more complicated. You will have to make networks calls to fetch your data from other microservices. But a network call is abstract and doesn't provide much more information other that we are probably going to use the internet protocol to talk to each other. This doesn't define the layer on top of them. Are we going to use raw TCP / UDP and serialize and deserialize the data ourselves ? What would that implies ? Do we rely on existing technology, such as HTTP ? Maybe, but how do you define your payloads ? Where exactly resides your data, or business logic ? Maybe doing synchronous calls doesn't really makes sense. Do we really need to wait for an email to be sent ? Doesn't this create a strong coupling, something we're trying to avoid ?
All of those questions, and so much more, is why you needed educated architect and developers in your team. The decisions and implementations you are going to take will be a set of trade-off, and you need to identify, for each of your microservices, what are the trade-offs you're willing to make. But what are the trade-offs you can make exactly, and what are the available solutions on the market that can help us resolve those problems ?

== What are the goals ==
Let's identify and talk about the main goals and characteristics we look for in a inter microservices communication. These will be concepts, and we are not yet diving into actual protocols and solutions, but we are rather taking a high level overview of every piece of the puzzle we will need to build up a sensible solution for our use cases.

=== Latency and performance ===
As mentioned previously, since we moved our services in different applications, we also added overhead for all of our communication between our services. Wether the applications runs in the same machine or not, the transport delay and overhead is now significant, and can't be ignored, especially when requesting something that does not need a lot of processing. When choosing a protocol, you need to think about the overhead it will introduce in term of latency, especially when the two services will be chatty.
You need to identify if your services need synchronous response, meaning that they can't continue to process a request until they have a response from another service. If this is the case, you probably want a transport protocol that is fast, that can keep connection open (so you don't have to handshake every time), and if you're going to send a batches of request at the same time, a protocol transport that can support multiplexing over the same connection. You don't want to end up open dozens of connections, as this is a significant overhead, on both services.
Opening, closing and multiplexing connections are far from the only aspects of performance. On top of your connection, you're going to transfer data. The way you encode this data is also very important for your overall performance. Indeed, more verbose encoding (self-describing one, like JSON) are heavy to transport, and ends up in more traffic on your connections. This might not be a big concern, depending on your deployment architecture, but you still need to take this in consideration.

Another downside of those verbose encoding, is the cost of serialization and deserialization. I've seen applications where 60 to 70 percents of the CPU load was encoding and decoding JSONs. This microservice was poorly designed, its main job was to handle huge batches of requests, all with a well-known request and responses structure. It was running on HTTP/1.1, which does not handle multiplexing (unlike its bigger brother, HTTP/2), with a REST API (using JSON as data structure). The performance were catastrophic, and switching to a protocol that supported multiplexing and that has a close to computer serialization, we were able to speed up significantly all the requests to this microservice, that became a bottleneck, while reducing replicas, clearly highlighting the cost related to choosing the wrong protocol for the job.

However, if you need to process data that might not be well-defined ahead of time or if you don't need to have this kind of performance, because your microservices will not be chatty at all, self-describing encoding can make it way easier to debug, since those tends to be human-readable, and does not cause a big performance hit when the volume is low.

