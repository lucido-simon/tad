= Inter microservices communication =

== Introduction ==
With microservices comes a set of new challenges. One of them is not only to think about the data getting transferred between your services and layer of applications, like with a monolith application, but also to think how this data is transferred. Indeed, it was relatively simple before. Since everything was running in a single application, all you had to do was a function call ! With microservices however, this becomes much more complicated. You will have to make networks calls to fetch your data from other microservices. But a network call is abstract and doesn't provide much more information other that we are probably going to use the internet protocol to talk to each other. This doesn't define the layer on top of them. Are we going to use raw TCP / UDP and serialize and deserialize the data ourselves ? What would that implies ? Do we rely on existing technology, such as HTTP ? Maybe, but how do you define your payloads ? Where exactly resides your data, or business logic ? Maybe doing synchronous calls doesn't really makes sense. Do we really need to wait for an email to be sent ? Doesn't this create a strong coupling, something we're trying to avoid ?
All of those questions, and so much more, is why you needed educated architect and developers in your team. The decisions and implementations you are going to take will be a set of trade-off, and you need to identify, for each of your microservices, what are the trade-offs you're willing to make. But what are the trade-offs you can make exactly, and what are the available solutions on the market that can help us resolve those problems ?

== What are the goals ==
Let's identify and talk about the main goals and characteristics we look for in a inter microservices communication. These will be concepts, and we are not yet diving into actual protocols and solutions, but we are rather taking a high level overview of every piece of the puzzle we will need to build up a sensible solution for our use cases.

=== Latency and performance ===
As mentioned previously, since we moved our services in different applications, we also added overhead for all of our communication between our services. Wether the applications runs in the same machine or not, the transport delay and overhead is now significant, and can't be ignored, especially when requesting something that does not need a lot of processing. When choosing a protocol, you need to think about the overhead it will introduce in term of latency, especially when the two services will be chatty.

You need to identify if your services need synchronous response, meaning that they can't continue to process a request until they have a response from another service. If this is the case, you probably want a transport protocol that is fast, that can keep connection open (so you don't have to handshake every time), and if you're going to send a batches of request at the same time, a protocol transport that can support multiplexing over the same connection. You don't want to end up open dozens of connections, as this is a significant overhead, on both of your services.
Opening, closing and multiplexing connections are far from the only aspects of performance. On top of your connection, you're going to transfer data. The way you encode this data is also very important for your overall performance. Indeed, more verbose encoding (self-describing one, like JSON) are heavy to transport, and ends up in more traffic on your connections. This might not be a big concern, depending on your deployment architecture, but you still need to take this in consideration.

Another downside of those verbose encoding, is the cost of serialization and deserialization. I've seen applications where 60 to 70 percents of the CPU load was encoding and decoding JSONs. This microservice was poorly designed, its main job was to handle huge batches of requests, all with a well-known request and responses structure. It was running on HTTP/1.1, which does not handle multiplexing (unlike its bigger brother, HTTP/2), with a REST API (using JSON as data structure). The performance were catastrophic, and switching to a protocol that supported multiplexing and that has a close to computer serialization, we were able to speed up significantly all the requests to this microservice, that became a bottleneck, while reducing replicas, clearly highlighting the cost related to choosing the wrong protocol for the job.

However, if you need to process data that might not be well-defined ahead of time or if you don't need to have this kind of performance, because your microservices will not be chatty at all, self-describing encoding can make it way easier to debug, since those tends to be human-readable, and does not cause a big performance hit when the volume is low.

<<<
=== Fault tolerance and resiliency ===
Fault tolerance and resiliency are important considerations in inter-microservice communication, as they help make sure that the overall system can continue to function properly even in the face of failures or errors. These concepts refer to the ability of a system to continue functioning in the face of failures or errors, and to recover from these failures in a timely and effective manner.

Fault tolerance is the ability of a system to continue functioning normally, or at least in a degraded mode, when one or more of its components fails or experiences an error. This is accomplished through the use of redundant components and failover mechanisms that allow the system to continue operating even if one or more components fail. In the context of inter-microservice communication, fault tolerance may involve the use of load balancers, message queues, and other mechanisms to make sure that messages are delivered to their intended recipients even if one or more microservices are offline or experiencing errors.

On the other hand, resiliency refers to the ability of a system to recover from failures or errors in a timely and effective manner. This may involve the use of techniques such as rolling updates, which allow a system to be updated or repaired without disrupting service, or the use of self-healing mechanisms that automatically detect and correct problems as they occur. In the context of inter-microservice communication, resiliency may involve the use of monitoring and alerting systems to detect problems and trigger corrective actions, as well as the use of recovery procedures to restore the system to a healthy state after a failure or error.

Together, fault tolerance and resiliency are essential for ensuring the reliability and uptime of a distributed system, particularly in cases where the system is critical for business operations or customer experience. By designing their inter-microservice communication systems to be fault tolerant and resilient, organizations can reduce the risk of service disruptions and improve their overall system reliability.

One way to achieve fault tolerance in inter-microservice communication is through the use of redundant communication channels. For example, if a microservice communicates with another microservice over a network connection, it might be advisable to have multiple network connections available in case one fails. This can help prevent a single point of failure in the system. This is mostly applicable on systems that communicates over the internet, and you need to design your architecture with this knowledge in mind.
Another way to achieve fault tolerance is through the use of circuit breakers. A circuit breaker is a pattern that allows a microservice to temporarily stop attempting to communicate with another microservice if it detects that the other microservice is experiencing problems. This can help prevent the microservice from becoming overloaded with failed requests, and can allow it to continue functioning properly even if the other microservice is experiencing issues. It has, however, some downsides :

* Increased complexity: Implementing circuit breakers can add complexity to a system, as it requires the development and maintenance of additional code.
* False positives: If the circuit breaker is too sensitive, it may trigger unnecessarily and cause a microservice to stop communicating with another microservice even when there is no actual problem. This can lead to false positives, which can cause disruptions in service and reduce overall system performance.
* Stale data: If a circuit breaker is triggered and a microservice stops communicating with another microservice, it may continue to use stale data until the circuit breaker is reset. This can lead to outdated or incorrect information being used, which can have negative consequences.
* Limited visibility: Circuit breakers can make it more difficult to diagnose and troubleshoot problems, as they can obscure the root cause of failures.
* Dependency on a single point of failure: While circuit breakers can help prevent a single point of failure from cascading through the system, they themselves can become a single point of failure if they are not implemented and maintained properly.

Overall, while circuit breakers can be a useful tool for improving fault tolerance and resiliency in inter-microservice communication, they should be carefully evaluated and used with caution to make sure that they do not cause more problems than they solve.

Resiliency in inter-microservice communication can be achieved through the use of retries and failover mechanisms. For example, if a microservice is unable to communicate with another microservice, it might retry the request a certain number of times before giving up. 
Retries refer to the process of repeating a request or operation in the event of a failure or error. Retries are commonly used in distributed systems to improve reliability and make sure that requests are successful even if there are temporary failures or errors.
There are several factors to consider when implementing retries in a distributed system. These include the number of retries to be attempted, the interval between retries, and the conditions under which retries should be attempted. It is important to balance the benefits of retries with the potential for increased load on the system and the risk of creating an infinite loop if the failure is not resolved.
Overall, retries are a useful tool for improving the reliability and uptime of a distributed system. By implementing retries in a smart and well-considered manner, we can improve the success rate of our requests and reduce the risk of service disruptions.

In addition to these proactive approaches, it is also important to have robust recovery procedures in place in case of more serious failures or errors. This may involve the use of backup systems and data, as well as well-defined recovery processes that can be followed in the event of a system failure.
