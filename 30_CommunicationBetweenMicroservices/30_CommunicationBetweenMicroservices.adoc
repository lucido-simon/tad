= Inter microservices communication =

== Introduction ==
With microservices comes a set of new challenges. One of them is not only to think about the data getting transferred between your services and layer of applications, like with a monolith application, but also to think how this data is transferred. Indeed, it was relatively simple before. Since everything was running in a single application, all you had to do was a function call ! With microservices however, this becomes much more complicated. You will have to make networks calls to fetch your data from other microservices. But a network call is abstract and doesn't provide much more information other that we are probably going to use the internet protocol to talk to each other. This doesn't define the layer on top of them. Are we going to use raw TCP / UDP and serialize and deserialize the data ourselves ? What would that implies ? Do we rely on existing technology, such as HTTP ? Maybe, but how do you define your payloads ? Where exactly resides your data, or business logic ? Maybe doing synchronous calls doesn't really makes sense. Do we really need to wait for an email to be sent ? Doesn't this create a strong coupling, something we're trying to avoid ?
All of those questions, and so much more, is why you needed educated architect and developers in your team. The decisions and implementations you are going to take will be a set of trade-off, and you need to identify, for each of your microservices, what are the trade-offs you're willing to make. But what are the trade-offs you can make exactly, and what are the available solutions on the market that can help us resolve those problems ?

== What are the goals ==
Let's identify and talk about the main goals and characteristics we look for in a inter microservices communication. These will be concepts, and we are not yet diving into actual protocols and solutions, but we are rather taking a high level overview of every piece of the puzzle we will need to build up a sensible solution for our use cases.

=== Latency and performance ===
As mentioned previously, since we moved our services in different applications, we also added overhead for all of our communication between our services. Wether the applications runs in the same machine or not, the transport delay and overhead is now significant, and can't be ignored, especially when requesting something that does not need a lot of processing. When choosing a protocol, you need to think about the overhead it will introduce in term of latency, especially when the two services will be chatty.

You need to identify if your services need synchronous response, meaning that they can't continue to process a request until they have a response from another service. If this is the case, you probably want a transport protocol that is fast, that can keep connection open (so you don't have to handshake every time), and if you're going to send a batches of request at the same time, a protocol transport that can support multiplexing over the same connection. You don't want to end up open dozens of connections, as this is a significant overhead, on both of your services.
Opening, closing and multiplexing connections are far from the only aspects of performance. On top of your connection, you're going to transfer data. The way you encode this data is also very important for your overall performance. Indeed, more verbose encoding (self-describing one, like JSON) are heavy to transport, and ends up in more traffic on your connections. This might not be a big concern, depending on your deployment architecture, but you still need to take this in consideration.

Another downside of those verbose encoding, is the cost of serialization and deserialization. I've seen applications where 60 to 70 percents of the CPU load was encoding and decoding JSONs. This microservice was poorly designed, its main job was to handle huge batches of requests, all with a well-known request and responses structure. It was running on HTTP/1.1, which does not handle multiplexing (unlike its bigger brother, HTTP/2), with a REST API (using JSON as data structure). The performance were catastrophic, and switching to a protocol that supported multiplexing and that has a close to computer serialization, we were able to speed up significantly all the requests to this microservice, that became a bottleneck, while reducing replicas, clearly highlighting the cost related to choosing the wrong protocol for the job.

However, if you need to process data that might not be well-defined ahead of time or if you don't need to have this kind of performance, because your microservices will not be chatty at all, self-describing encoding can make it way easier to debug, since those tends to be human-readable, and does not cause a big performance hit when the volume is low.

<<<
=== Fault tolerance and resiliency ===
Fault tolerance and resiliency are important considerations in inter-microservice communication, as they help make sure that the overall system can continue to function properly even in the face of failures or errors. These concepts refer to the ability of a system to continue functioning in the face of failures or errors, and to recover from these failures in a timely and effective manner.

Fault tolerance is the ability of a system to continue functioning normally, or at least in a degraded mode, when one or more of its components fails or experiences an error. This is accomplished through the use of redundant components and failover mechanisms that allow the system to continue operating even if one or more components fail. In the context of inter-microservice communication, fault tolerance may involve the use of load balancers, message queues, and other mechanisms to make sure that messages are delivered to their intended recipients even if one or more microservices are offline or experiencing errors.

On the other hand, resiliency refers to the ability of a system to recover from failures or errors in a timely and effective manner. This may involve the use of techniques such as rolling updates, which allow a system to be updated or repaired without disrupting service, or the use of self-healing mechanisms that automatically detect and correct problems as they occur. In the context of inter-microservice communication, resiliency may involve the use of monitoring and alerting systems to detect problems and trigger corrective actions, as well as the use of recovery procedures to restore the system to a healthy state after a failure or error.

Together, fault tolerance and resiliency are essential for ensuring the reliability and uptime of a distributed system, particularly in cases where the system is critical for business operations or customer experience. By designing their inter-microservice communication systems to be fault tolerant and resilient, organizations can reduce the risk of service disruptions and improve their overall system reliability.

One way to achieve fault tolerance in inter-microservice communication is through the use of redundant communication channels. For example, if a microservice communicates with another microservice over a network connection, it might be advisable to have multiple network connections available in case one fails. This can help prevent a single point of failure in the system. This is mostly applicable on systems that communicates over the internet, and you need to design your architecture with this knowledge in mind.
Another way to achieve fault tolerance is through the use of circuit breakers. A circuit breaker is a pattern that allows a microservice to temporarily stop attempting to communicate with another microservice if it detects that the other microservice is experiencing problems. This can help prevent the microservice from becoming overloaded with failed requests, and can allow it to continue functioning properly even if the other microservice is experiencing issues. It has, however, some downsides :

* Increased complexity: Implementing circuit breakers can add complexity to a system, as it requires the development and maintenance of additional code.
* False positives: If the circuit breaker is too sensitive, it may trigger unnecessarily and cause a microservice to stop communicating with another microservice even when there is no actual problem. This can lead to false positives, which can cause disruptions in service and reduce overall system performance.
* Stale data: If a circuit breaker is triggered and a microservice stops communicating with another microservice, it may continue to use stale data until the circuit breaker is reset. This can lead to outdated or incorrect information being used, which can have negative consequences.
* Limited visibility: Circuit breakers can make it more difficult to diagnose and troubleshoot problems, as they can obscure the root cause of failures.
* Dependency on a single point of failure: While circuit breakers can help prevent a single point of failure from cascading through the system, they themselves can become a single point of failure if they are not implemented and maintained properly.

Overall, while circuit breakers can be a useful tool for improving fault tolerance and resiliency in inter-microservice communication, they should be carefully evaluated and used with caution to make sure that they do not cause more problems than they solve.

Resiliency in inter-microservice communication can be achieved through the use of retries and failover mechanisms. For example, if a microservice is unable to communicate with another microservice, it might retry the request a certain number of times before giving up. 
Retries refer to the process of repeating a request or operation in the event of a failure or error. Retries are commonly used in distributed systems to improve reliability and make sure that requests are successful even if there are temporary failures or errors.
There are several factors to consider when implementing retries in a distributed system. These include the number of retries to be attempted, the interval between retries, and the conditions under which retries should be attempted. It is important to balance the benefits of retries with the potential for increased load on the system and the risk of creating an infinite loop if the failure is not resolved.
Overall, retries are a useful tool for improving the reliability and uptime of a distributed system. By implementing retries in a smart and well-considered manner, we can improve the success rate of our requests and reduce the risk of service disruptions.

In addition to these proactive approaches, it is also important to have robust recovery procedures in place in case of more serious failures or errors. This may involve the use of backup systems and data, as well as well-defined recovery processes that can be followed in the event of a system failure.

=== Decoupling ===
One of the main benefit of a good microservice architecture, is how you can make your microservices independent from one another, at the development stage but also when running. By decoupling the services, they become more independent and can be developed, tested, and deployed independently of each other. This makes it easier to make changes to one service without affecting the others, as well as allowing for more efficient and flexible development processes. You should aim at making your microservices the least reliant on others as possible. If your microservices can't function without another one, it should be a sign that those two microservices should be merged into one. 

One side effect of decoupling your microservices, is that they can be scale independently. This means, however, that your inter-microservice communication implementation should not relies on a specific instance, or make any assumptions about the availability and state of another service, since others microservices will be scaled up and down based on the current load of your application. This means you need to be able to adapt to the current state of your services, meaning you have state somewhere in your system. Since handling state within an application should be avoided, this is usually done at the operation layer. This provides multiple benefits:

* Your applications can be deployed in a variety of environment, and is not tied to your current system.
* Operation-layer solutions usually focuses on this specific problem. This means there are solutions that are robust, reliant and with a large set of features already available to be used.
* Since communication between your microservices should be standardized, adding an implementation in each of your microservice adds a lot of overhead.

This is usually done by using service discovery. In a Kubernetes environment, service discovery refers to the process of locating and communicating with services running in the cluster. There are two main approaches to service discovery in Kubernetes: client-side discovery and server-side discovery.

Client-side discovery involves the use of a client library that knows how to discover and communicate with the various services in the cluster. The client library abstracts the details of service discovery and communication, allowing the application to focus on its business logic. This, however, as discussed above, makes your application architecture-aware, in the sense that it has to handles operation specific implementation. If your microservices are written in different languages, or use a different framework or version from one another, you will have multiple library to maintain, which can become cumbersome.

Server-side discovery involves the use of a central service or component that is responsible for managing the registration and discovery of services in the cluster. The services communicate with this central component to register themselves and locate other services they need to communicate with. This, however, introduce a single point of failure. If your central component were to fail, the entire system may become unavailable, and you need to have a strong failover system to take over when this component eventually fails. This single point of failure is also a single point of communication for all your microservices, that it becomes much more easier for your developers to interact with your system.
Since you have an additional hop, this also increases latency. But with this additional hop, we also get more centralized monitoring options, easily control access policies and authentication between microservices. 

=== Security ===
In a microservice architecture, security in inter-service communication is important because services are decoupled and communicate with each other over networks. This means that there is a higher risk of unauthorized access to data or services and the potential for attacks such as man-in-the-middle or replay attacks. There are several way to secure inter-service communication in a microservice architecture:

* Use secure communication protocols. Services can communicate with each other using secure protocols such as HTTPS (or more broadly TLS). It encrypts the data transmitted between services, making it more difficult for attackers to intercept and read the data.
* Implement authentication and authorization. Services can authenticate each other using techniques such as mutual TLS (mTLS) or JSON Web Tokens (JWT). This ensures that only authorized services can communicate with each other.
* Use service a layer of infrastructure that sits between services and handles communication between them. This can be typically handled by a service mesh (more on them later), and can provide features such as mTLS, rate limiting, and request tracing.
* Network segmentation: Divide your network into smaller, isolated segments. This helps preventing unauthorized access to services and data by limiting the ability of attackers to move laterally within your network.
* API Gateways: It's a reverse proxy that sits between clients and services, and acts as a single point of entry for all incoming requests. It usually also implements authentication, authorization, rate limiting, is the entry point for your traces, and some also serve as a service discovery registerer.

=== System agnosticism ===
The last goal I would like to talk about is system agnosticism. A good communication protocol should not rely on a specific underlying technology, and should be usable whatever the implementation you make of them. Java's Object Streams is an good example of a bad idea, since they're not agnostic (and have a ton more problems totally out of this scope).
What the underlying constraint is, is to choose a protocol that can be serialized and deserialized easily, whatever the stack you use. Most solutions out there nowadays are system agnostic (even if some languages are easier to work with for some), such as JSON or gRPC.
Once again, the idea with microservices is that you have multiples, small-sized, independent systems. If you introduce technologies that makes assumptions about what's running on the other end of your connection, it means that for your system to work, the other end actually has to follow those assumptions. You're putting yourself into handcuffs, and are tying your microservices together.

== The options, protocol layer ==
Now that we defined the main goals of an inter-service communication, we'll look into what are the existing solutions, and which makes sense in our Polycode architecture. We'll start by talking about solutions at the protocol layer, meaning we will look at solutions that defines how data is structured inside the messages that are sent, what type of messages can be sent, etc..

=== GraphQL ===
I'll start by talking about a solution that doesn't make sense to me. While researching, I've came across solutions that were using GraphQL as a inter-microservice communication protocol. I would argue that GraphQL is made to sit between the frontend and the entry point of your API, most likely with an API Gateway as your GraphQL Server, and your microservices as GraphQL Resolvers. Its front-facing features are great, and you can filter out the fields that should remain internal to your system. 
However, when working within your microservices, you should not have to strip fields and data. You should not rely on a GraphQL Server to resolve your requests for your, as this introduces a massive single point of failure. All your microservices become dependant on this service, and if it were to go down, all your infrastructure would go done as well. Your microservices should be able to talk to one another through a well-defined API, with contracts that does not change over time. With GraphQL, you have no decoupling, scalability will be limited by the scale of your GraphQL server (which needs to do all the heavy-lifting, not just passing-through requests), you have no option for resiliency and your failover options are limited to implementing another communication protocol, which you should probably do in the first place.

GraphQL is not the right tool for inter-service communication, and I'll strongly suggest avoiding using it as your communication protocol. I would not use GraphQL is any of the inter-microservice communication protocol in Polycode.

=== Asynchronous communications ===
While we are not allowed to use them in our solutions, asynchronous communications have become a standard for some types of inter-microservice communication. Message queues solutions such as AMQP or Kafka have demonstrated how well they can handle heavy-load, and how they are useful at distributing events through your domain. Their main use case is to broadcast events that need to be handled eventually, but doesn't need an immediate response for the continuation of the current process. Sending a confirmation email, is a great example. You need it to be done eventually, but it is usually not critical to make sure the email was sent before continuing. 
Another big advantage of message queues is that they offer a total decoupling of microservices. A service does not need to know who will handle the event. This comes with the side effect of a great resiliency and fault tolerance, since the events are stored, even if the micro service consuming them is down, so when it will eventually gets started back up, it can consume the requests it missed and have the operations brought back to normal, during a totally transparent process to all the others microservices.
However, messages queues should not be used for critical and synchronous operations, such as a transactions, as it is often tedious to have feedback when your request has been processed, and you can not afford putting a whole transaction in standby waiting for something to eventually be processed. You want a fast, synchronous response, that fails if the service that is handling one of your requests fails to respond correctly.
If I was allowed to use them in my architecture, I would use them for most of my domain events, such as sending emails for example.

=== REST API ===
REST, or Representational State Transfer, is a popular architectural style for building web-based APIs (Application Programming Interfaces). One of the main advantages of using REST for inter-microservice communication is that it is a widely adopted standard, which means that there are many tools and libraries available for building and consuming REST APIs. This can make it easier to integrate with third-party services and to build scalable, reliable systems.

Another advantage of REST is that it is based on the HTTP protocol, which is a well-established and widely supported protocol for communication on the web. This means that REST APIs can be easily accessed from any platform or language that supports HTTP, which is basically everything. Additionally, HTTP has a number of built-in features, such as caching, security, and error handling, which can be leveraged when building REST APIs.

However, there are also some disadvantages to using REST for inter-microservice communication. One potential issue is that REST relies on a stateless request-response model, which means that each request must contain all of the necessary information for the server to understand and fulfill the request. This can make it difficult to maintain context or state between multiple requests, which can be an issue if your microservices need to communicate complex data or maintain a stateful connection. This, however, might be seen as a blessing in disguise. Having a protocol that doesn't allow for stateful connection, forces the hand of developers to create stateless systems, that are easily scaled up, but also easily scaled down. Both of those problems are difficult to tackle in stateful environments, and this is why you often see a push towards making stateless application.

Another disadvantage of REST is that it can be difficult to ensure that the API is being used correctly, as there are no strict rules governing how the API should be implemented. This can lead to issues with compatibility and maintainability, as different teams or developers may implement the API in different ways. Additionally, REST APIs can be difficult to version and maintain over time, as changes to the API may break existing client implementations.

==== Pull vs Push model ====
Another thing to consider, is the flow of your data within your system. This is the push vs pull model:

In the push model, the server pushes data to the client as it becomes available. This is typically achieved using a technique called long polling, in which the client sends a request to the server and the server holds the request open until it has new data to send to the client. The client can then receive the data and send another request to the server to get more data as needed.

The pull model, on the other hand, involves the client pulling data from the server as needed. In this model, the client sends a request to the server to retrieve a specific piece of data, and the server responds with the requested data. The client is responsible for initiating each request and can decide when and how often to request data from the server.

Both the push and pull models have their own advantages and disadvantages. The push model is useful for scenarios where the server needs to send data to the client in real-time, as it allows the server to proactively push data to the client as soon as it becomes available. However, it can also be resource-intensive for the server, as it requires maintaining open connections with multiple clients.

The pull model, on the other hand, is more efficient for the server, as it only needs to respond to requests from the client as they are made. However, it requires the client to actively request data from the server, which can make it less suitable for scenarios where real-time data updates are required.

Ultimately, the choice between the push and pull model will depend on the specific needs of your application and the data exchange patterns between the client and server. However, implementing a push model with a REST API is cumbersome, and long polling is a hacky way of using HTTP requests to reverse the flow of data. If your use case is within this use case, or if you need duplex, using REST might not be the right choice.

==== Performance ====
There are several factors to take into consideration when talking about REST and performance. First, let's talk about multiplexing:

Multiplexing in REST is typically achieved using a technique called HTTP/2 multiplexing, which is supported by the HTTP/2 protocol. HTTP/2 multiplexing allows multiple requests and responses to be sent over a single connection in parallel, rather than having to wait for each request to complete before sending the next one. This can help to improve the performance of a REST API by reducing the overhead associated with establishing and tearing down separate connections for each request.

To use multiplexing in a REST API, the client and server must both support HTTP/2 and the client must initiate the connection using the HTTP/2 protocol. The client can then send multiple requests over the same connection, and the server can respond to each request as it is received.

Multiplexing in HTTP/2 comes with several features to improve performance, such as solving Head Of Line Blocking at the HTTP Level, but the problem still persists at the TCP Level. This is why HTTP/3 is taking a whole new approach, but this is out of the scope of this section. I would highly suggest to communicate through HTTP/2 for all of your REST communication, especially when you have high traffic between your microservices.

A second factor to take into account when talking about performance and REST, is that REST API needs to serialize and deserialize payloads in self-describing formats, typically JSON. I will assume the use of JSON for the rest of this paragraph.

Both JSON serialization and deserialization can add overhead to the performance of a REST API, as they require additional processing to convert the data between different formats. This overhead can be particularly noticeable for large payloads or for scenarios where high volumes of data are being transferred.

One way to reduce the overhead of JSON serialization and deserialization in a REST API is to optimize the design of the API to minimize the amount of data that needs to be transferred with each request and response. This can involve using compact data structures and minimizing the number of unnecessary fields or metadata included in the payload.

In comparison, gRPC (Google Remote Procedure Call) uses a binary encoding for data transfer, which can be more efficient than the text-based encoding used by JSON. This can make gRPC more performant than REST, particularly for scenarios where high volumes of data need to be transferred or when low latencies are important. However, it is important to note that the performance of any specific implementation of REST or gRPC will depend on a number of factors, including the hardware and software infrastructure used to host the API, the design of the API itself, and the volume and complexity of the requests being made. We will talk more about gRPC in the next part.

Overall, REST is a powerful and widely used tool for building APIs and facilitating inter-microservice communication, but it is important to carefully consider the potential disadvantages and limitations when deciding whether to use it in your specific use case.

=== gRPC ===
In this part, I'm going to dive into what is gRPC, why is it becoming a more and more popular option for inter-microservice communication and what are the pros and cons of using it. gRPC is a high-performance, open-source framework for building remote procedure call (RPC) APIs. It is based on the Protocol Buffers data serialization format, and uses HTTP/2 for transport. gRPC enables efficient communication between microservices, with support for bi-directional streaming, flow control, and flow-limited concurrency. Additionally, gRPC provides built-in support for load balancing, health checking, and error handling. Overall, gRPC is a powerful tool for building scalable and efficient inter-microservice communication systems.

Unlike REST, which operates in a resource-based manner, gRPC takes more of a function call approach. You define services, that exposes functions that you can call in multiple manners. There are four main ways to define a "function":

* Unary RPC: This is the simplest form of RPC, in which the client sends a single request to the server and the server responds with a single response. This type of RPC is typically used for simple, one-time queries or commands.
* Server streaming RPC: In this type of RPC, the client sends a single request to the server, and the server responds with a stream of responses. This is useful for situations where the server needs to send a large amount of data to the client, or fetching them on the fly.
* Client streaming RPC: In this type of RPC, the client sends a stream of requests to the server, and the server responds with a single response. This is useful for situations where the client needs to send a large amount of data to the server.
* Bidirectional streaming RPC: In this type of RPC, both the client and the server can send and receive streams of data. This is useful for situations where real-time communication is required, or when processing can begin as soon as with get our first piece of a payload.

==== Efficiency ====
gRPC is known for its high performance and efficiency, which are largely due to the use of Protocol Buffers as the data serialization format and HTTP/2 as the transport protocol.

Protocol Buffers, also known as protobuf, is a binary data serialization format that is smaller and faster than other formats such as JSON or XML. It is designed to be language and platform neutral, allowing for easy cross-language communication between microservices.

HTTP/2 is a binary protocol that is designed to be faster and more efficient than HTTP/1.1, the previous version of the protocol. It allows for multiplexing multiple requests over a single connection, reducing the overhead of opening and closing connections for each request. HTTP/2 also supports server push, which allows the server to proactively send data to the client without a request, further reducing round-trip time.

Furthermore, gRPC provides more advanced features that helps improving the overall performance of the system, if well-used, such as request cancellation, health checking, error handling or load balancing.

Overall, the use of Protocol Buffers and HTTP/2, along with the features provided by gRPC, makes it a highly efficient and effective tool for building high-performance inter-microservices communication systems.

==== Multiplexing ====
Multiplexing is a technique used to send multiple requests and responses over a single connection. 

In gRPC, multiplexing is achieved by using a single HTTP/2 connection to send multiple requests and responses. Each request and response is identified by a unique stream ID, which allows the server to distinguish between different requests and responses. This opens the door for multiple requests and responses to be sent over a single connection at the same time, reducing the overhead of opening and closing connections for each request.

This feature of multiplexing in HTTP/2, and thus gRPC, improves the efficiency of communication between microservices as it allows multiple requests and responses to be sent over a single connection, reducing the overhead of opening and closing connections for each request. It also allows for bi-directional streaming, which enables real-time communication between the client and server.

Additionally, the use of flow control (which is defined in the https://httpwg.org/specs/rfc7540.html#FlowControl[HTTP/2 RFC]) in gRPC ensures that the server does not become overloaded with too many requests, thus improving the overall performance of the communication. This is done by limiting the number of requests that can be sent over a single connection, and by controlling the rate at which data can be sent for each flow (stream, request) to the connection.
HTTP/2 also defines the concept of stream priority, meaning we can prioritize certain gRPC calls, if the need arise.

There are others very interesting features that HTTP/2 defines that allows for even better performance, such as stream dependencies or reprioritization. Since gRPC was conceived with HTTP/2 in mind, a lot of these features can be leveraged. I recommend reading the https://httpwg.org/specs/rfc7540.html[specifications] for more details.

This ensures that resources are used efficiently and that the server is not overwhelmed.

==== Protobuf ====
Protocol Buffers (protobuf) is a data serialization format developed by Google. It is designed to be small, fast, and efficient, and is used to transmit data between different systems and programming languages.

Protobuf uses a language and platform-neutral binary format to serialize data, making it smaller and usually faster to serialize/deserialize than JSON. Additionally, protobuf includes a code generation tool that generates data access classes for various programming languages, making it easy to work with protobuf data in those languages.

One of the key advantages of using protobuf over other binary serialization format is its ability to evolve over time. When a field is added or removed from the data structure, older clients can still correctly parse the data they receive, and newer clients can correctly parse the older data. This allows for a more gradual rollout of updates, and reduces the risk of breaking existing clients.

Protobuf also supports the definition of message types, services, and enums, which can be used to define the structure of the data and the methods that are available to interact with it. This makes it easy to understand the structure of the data, and how it should be used.

Protobuf is not self-describing, meaning that you can't discover how a message should be parsed when receiving it. This means you need to know that beforehand, that's why you need your protobuf definition for both your client and server. This comes with its advantages and disadvantages, primarily having a well-defined contract for your inter-microservice communication (reducing the risk of misunderstanding and informal coordination between teams), at the cost of a more rigid develop constraints. This makes it robust and improves the trust you can have in your production environments, but make it more cumbersome to develop with. Changing your APIs becomes hard, and payloads are binary-encoded, which make it harder for prototyping and debugging. It's a tradeoff that more and more organizations are willing to make.

Overall, protobuf is a powerful and efficient data serialization format that is used for a wide range of applications. It is widely used in high-performance systems, including distributed systems and microservices. gRPC uses protobuf as the data serialization format, which makes it efficient for communication between microservices.

==== xDS Support ====
Cementing gRPC relationship in inter-microservice communication, it is gradually implementing the xDS protocol, with a great set a features already production-ready. xDS (Extensible Dynamic Configuration) is a set of APIs and protocols for configuring and managing services in a distributed system. xDS provides a common interface for configuring and managing services, regardless of their underlying technology. It is widely used in service mesh architectures and service discovery mechanisms, and it is the foundation of service management in Envoy (in fact, xDS was created by the Envoy team), a popular open-source and service proxy. To put it more simply, it allows for management of service meshes.

We are going to dive deeper into service meshes later in this section, but for now it is important to point out that, by making gRPC xDS-ready, it is enabling for proxyless gRPC service meshes, which is a huge deal in term of performance, but also security. It means that we don't need to spawn a proxy for each of our microservice to intercept all the communications our microservice does, redirecting them to the correct endpoint. We reduce a lot of performance overhead, both in term of IO throughput and CPU usage (we are reducing the hops between user space and kernel space, although eBPF is starting to help quite a lot here) and memory footprint. We also greatly reduce the complexity of our architecture. This also allows to have a true end-to-end encryption, since TLS termination is done in the application, and not at the proxy.

You can keep track of the current state of xDS in gRPC https://grpc.github.io/grpc/core/md_doc_grpc_xds_features.html[here]. Keep in mind that support might vary depending on the language you are using, since not all libraries are up-to-speed with latest gRPC specifications.

Overall, having the possibility to directly integrate with xDS opens for a world of possibilities, pushing even more the efficiency of this protocol.

==== Gateways ====
Although not in the scope of this section, I want to talk about one drawbacks that some see with using gRPC - even though it is not really on to me. gRPC can't be used in the browser. This is due to multiple technical factors, mainly around the freedom that web browsers give to their Javascript sandbox around the way they can manipulate raw HTTP packets. To put it simply, they can't. There is no way to force the usage of HTTP/2, and no way to dictate how streams should be handled within an HTTP/2 connection.

gRPC was never meant to be a communication protocol between your frontend and your backend. This is why I'm suggesting that it is not really a problem. However, this implies that you need a layer somewhere, that takes your frontend requests, and "translates" them to appropriate gRPC protocol. The most popular options are to have your microservice serving both a gRPC and a REST API (for internet-facing microservices) or to use gateways. I would recommend using the latest, since they allow you to have a single entry point into your system, although this introduce a single point of failure. You gain the benefits of having centralized control, a easier time introducing routing and load balancing, centralize your authentication and authorization and not have to deal with that in your microservices, caching and an single entry point in your service-mesh, if you have one. This however introduces additional hops to your request, additional overhead to your system and, as mentioned previously, a single point of failure.

It's important to evaluate the tradeoffs and decide wether the benefits outweigh the drawbacks in your use case and their specific requirements, while foreshadowing your architecture evolution, as with everything in this domain.

==== Support ====
The last thing I want to touch on, which is important factor whatever the solution, is the support and adoption. gRPC has gained significant popularity and community adoption in recent years, which is a trend that is forecasted to continue. It is an actively developed open-source project, with contributions from a large community of developers. It is supported by Google and has a growing ecosystem of third-party libraries and tools. gRPC provides several libraries and tools to help developers work with the gRPC framework, such as the Protocol Buffer Compiler (protoc), the gRPC C++, Java, Go and C# libraries, and the grpc-gateway for building RESTful APIs on top of gRPC services. Protoc can be configured to compile the boilerplate code for a wide variety of languages, while being highly customizable, allowing for different code generation per language, generating for a specific framework, for example.

==== gRPC, wrapped up ====
gRPC seems to come with a lot of benefits for few drawbacks when it comes to inter-microservice communication. It is now well-established in robust distributed system and is becoming a de-facto protocol to use as soon as you need to have some kind of performance or costs requirements for medium to large scale systems. It continues to develop on its microservice-oriented features such as xDS, and is now too big to fail in the upcoming years due to its sheer adoption in major companies, like Google, who are the project-owner and active maintainer of it.

== The options, operation layer ==
We've talked about what protocol we could used to communicate between our microservice. However, it is important to remember that our microservices are going to evolve in a distributed system, with most likely a layer below them to manage them, such as Kubernetes. This layer comes with new options on managing your inter-microservice communication, since they abstract away network concerns. We can put systems in place to remove complexity from our application layer, that should focus on business logic, and move it to our operation layer, where we can manage some aspects about how our system is ran.

In this chapter, we are going to explore a few options we have at this layer, diving deeper in concepts we've touch on previously, such as services mesh. I am making the assumptions that we are running our applications in a Kubernetes environments, since it is effectively the only solution you orchestrate your microservices at a large scale nowadays.

=== Kubernetes' service discovery ===
Kubernetes' service discovery is a feature that allows pods and services to discover and communicate with each other within a Kubernetes cluster. It is an important aspect of service-to-service communication in a microservices architecture.

In Kubernetes, services are used to expose pods to the network and to other pods within the cluster. Each service is assigned a unique IP address and a DNS name, and is exposed on a specific port. Pods can communicate with a service using its IP address and port, and can use the service's DNS name to communicate with it if the DNS service is enabled.

Kubernetes also provides built-in support for service discovery using the Kubernetes DNS service. When the DNS service is enabled, each pod is automatically configured to use the DNS service to resolve service names to IP addresses. This allows pods to communicate with services using their DNS names, rather than having to know their IP addresses and ports.

This means that, instead of having to keep a list of every IPs of your replicas for a particulate microservice, have only a single hostname, which will be resolved to a service that corresponds to an entry point for all of your pods for that microservice. It allows for simple load-balancing, dynamically adding and removing pods (enabling scalability), port-mapping as well as some more advanced features.

Overall, Kubernetes' service discovery is a powerful feature that allows pods and services to discover and communicate with each other within a Kubernetes cluster, it provides a built-in DNS service making it easy for pods and services to communicate with each other in a microservices architecture. It is usually sufficient for basic use-case, although the simplicity in its design makes it unable to have more advanced features that you could enable by introducing additional hops to operational applications, such as proxies. That's where service meshes come in.

=== Service mesh ===
A service mesh is a dedicated infrastructure layer for service-to-service communication in a microservices architecture. It is designed to manage the complexity of service-to-service communication, and to provide features such as traffic management, service discovery, load balancing, and security.

A service mesh typically consists of a set of proxies, known as "sidecar proxies", that are deployed alongside each service instance. These proxies handle service-to-service communication, and provide features such as traffic management, service discovery, load balancing, and security. The proxies are deployed through Kubernetes configuration, and are managed using a central control plane.

The control plane is responsible for configuring and managing the proxies, and can be used to perform tasks such as traffic management, service discovery, and security. The control plane can also be used to monitor the health and performance of the services and proxies, and to provide detailed visibility into service-to-service communication.

Picture are worth a thousand words, so here is a (very simplified) overview about how service meshes operates in a Kubernetes context:

image::30_CommunicationBetweenMicroservices/30_Service_Mesh_Pictured.png[]

By introducing proxies into the mix, we gain significant control about how to manage our inter-microservice communication. This allows us to introduce encryption for packets transiting through the cluster, usually with mTLS. Traffic on the internet is usually encrypted with TLS, where a site you visit prove their identity by providing a certificate, that you can see like an ID Card. Using this certificate, the client can verify and trust the site, and initiate a negotiation with the server, to secure the communication via encryption. Typically, the user needs to authenticate with a username and password to access protected resources. mTLS keep the core of this process, except that the TLS negotiation is done both ways. This means that both microservices can trust that the other is how they say they are, effectively the authentication process at the TLS level. During this process, they also negotiate a secure way to communicate.

What is very important to identify here, is that the identification is done at the session level, and the communication can be done normally after. There is no need to interfere with layer 7 data, since the authentication and encryption is done at lower level. Layer 7 is the application layer, so you can let your application freely send whatever data they might want to send.

Service meshes also provides way to implements resiliency and fault-tolerance features, such as retries and circuit-breakers. By having a proxy in front of each microservices, you can introduce advanced rules that dictates where packets goes, based on previous information, such as wether previous requests failed or not. Having this state and history, enables automatically sending new requests when previous one failed (retries), and identifying failing microservices and stop sending requests to this microservice for a while (circuit-breakers).

You can also test the robustness of your system, by injecting voluntary faults. This helps identifying bottlenecks and problems, before they actually arise. You basically have freedom about how you route your packets, with different solutions providing more or less features. This also allows real-time and precise observability about the state of your networks and packets traveling through them, once again at the operation layer, with no need to modify your application code.

To sum up, service meshes provide a transparent infrastructure layer for service-to-service communication, which allows developers to focus on building business logic, while the service mesh takes care of managing the complexity of service-to-service communication. The most popular service meshes are Istio, Linkerd and Consul. We are going to explore all three of them.

==== Consul ====
Consul is a service discovery and configuration tool developed by Hashicorp. It offers several features such as service discovery, health checking, key-value storage, load balancing, DNS interface, ACL system, event system, web UI and API. It is designed to be simple to operate, can be run as a single binary, doesn't require a separate data store, and can be run in a cluster for automatic failover. It is widely adopted and supported by a large community, it is also supported by many cloud providers and can be easily integrated with other Hashicorp tools. Consul Connect is a service mesh feature that allows for secure communication between services without requiring any code changes.

Consul connect uses Envoy under the hood, meaning that it communicates through the aforementioned xDS protocol. Where Consul Connects really shines is its integration with Consul, and the overall Hashicorp ecosystem. Configuration is done through a key-value store, making it easy to modify and configure on the fly. Consul Connect focuses on ease of use, with a nice UI coming bundled up with the solution, allowing you to easily edit your configuration and see the state of you system.

However, it can lack in advanced features and is considered as a solution coming with a lot of resource overhead.

==== Istio ====
Next up, I want to talk about Istio. Istio seems to be the more popular solution in production environment, thanks to its set of features and extensibility. Istio currently provides the biggest toolkit, with circuit breaking, retires and timeouts (both path and method based), fault injection, delay injection, header and path based traffic splits, percentage based traffic splits, mTLS, RBAC and much more. Istio takes a declarative approach, meaning that configuration is done through the editing of Kubernetes' Custom Resource Definitions (CRDs). This has proved to be an easier way to automate deployments and configuration with existing infrastructure-as-code tools like Terraform.

Envoy is also based on Envoy for its proxies, but is currently experimenting (with great results, although not production-ready) proxyless gRPC service mesh, by using the integration of the xDS protocol within gRPC. Istio's control plane can communicate directly with your gRPC application to configure it and route the traffic accordingly. This reduces drastically the performance overhead that comes with spawning an envoy proxy for each of your pods replicas. Istio also supports advanced routing for WebSockets and MongoDB.

Istio also comes with an UI that you can install that provides a very comprehensive real-time monitoring tool for your system. It is open-source and community-driven, integrated with most of the managed cloud ecosystem, and initiated by Google IBM and Lyft.

However, Istio might be the most overwhelming service mesh, making it the most difficult to manage and maintain. It can be challenging for teams that does not have the resources to explore the possibilities and configuration of Istio.

==== LinkerD ====
Another service mesh option, that is growing in popularity in recent years is LinkerD. It is fully open source, and a cloud native computing foundation (CNCF) project. LinkerD is designed to be non-invasive and is optimized for performance. It is really easy to adopt, and its sidecars proxies are very light and performant. It is an in-house "micro-proxy" written in Rust, but comes with a complete feature set that you can expect with services meshes, such as metrics, L4 and L7 advanced proxies, mTLS (by default !), retries, timeouts. The list of features is growing, with circuit breakers planned for the near future, for example.

However, I couldn't find a way to add a layer of compatibility with xDS, making it unusable with solutions that are developed around this protocol. This can be a major drawbacks depending on your needs.

==== Cilium ====
Lastly, I want to talk about cilium, which makes use of an interesting feature of the Linux kernel: eBPF. eBPF is a way to run your own code in a kernel sandbox. You can add and configure kernel features at runtime, allowing for example, to have some advanced routing logic. You can see where this is going, Cilium takes advantage of this feature to configure load balancing, mTLS and other advanced networking implementations at the eBPF layer, effectively never leaving the kernel, and avoiding two contexts switching between kernel space and user space in the process.

eBPF programs are executed in a sandbox, running on a JIT compiler. This performance are great, on the same order of magnitude as the kernel compiled code. More importantly, we have a very minimal memory and processing overhead, really squeezing out all the performance you can get from your resources, while still having service meshes' advantages.

Cilium is known to be quite difficult to configure, with a lot of custom resource definitions to set up for enabling features, such as circuit breaking or retries. It can also lack some features such as fault injection. Cilium can also work with Envoy, but I would argue that if you were to use cilium with envoy, you are better off using another service mesh. I would advise using cilium when you have strong performance requirements.

As always, the best solutions depends on your specific requirements. Heavy performance constraints might directs you towards LinkerD or Cilium, complex and complete observability and configuration features towards Istio and ease of use towards Consul Connect, for example. Now that we have skimmed over the options available, we are going to take a closer look at what would make sense, and what are the tradeoffs we are willing to make for Polycode.

== Polycode ==
With a better understanding of the solutions available, and with the overall concepts down, we are now going to talk about Polycode. This paper is about migrating Polycode to microservices after all. Firstly, I'm going to talk about what I suggest we use as technologies in our future Polycode architecture, and why those choices makes sense to me, by explaining my thoughts. I'm then going to show you some diagrams to better illustrate how my implementation would look like, as well as to make sure that the inner working of the system is understood. Lastly, I'm going to introduce you to a proof of concept using the technologies I chose, to show how it can be implemented.

=== My suggestions ===
Let's talk about technologies choices, starting with gRPC.

==== gRPC ====
I suggest that we use gRPC for all our inter-microservices communication. There are thoughts behind this suggestion, let's get into them.

To begin with, let's talk from a technical standpoint. On paper, gRPC is efficient, fast, provides a good developer experience and integrates natively with xDS. Performance is not an absolute requirement in our use case, but it is always important to keep it in mind when designing your system. Efficiency implicitly means a better usage of your resources, which are more often than not costly. This is not our case, but it is something to keep in mind when making your decisions. You might want to test the performance benefits of switching to gRPC in your environments to have some numbers you can compare and make a better educated decisions. https://github.com/lucido-simon/rust-grpc_vs_rest[I have a repo that provides some very rudimentary code] to test the performance benefits of using gRPC vs REST written in Rust. Beware, it is not documented, although very simple. It might be useful as some boilerplate to build your own test. As a reminder, gRPC efficiency does not only come from raw performance, but also from multiplexing and what comes with HTTP/2 altogether. 

Another major benefit of using gRPC is the duplex communication that it enables, both with unary or streaming request/responses. This can be very useful and important in a lot of scenarios. It enables, for example, streaming standard output and errors from the runner to the submission microservice, that can then stream this to the end user using websockets. Use case may arise, and using gRPC future-proofs our communication protocol.

I also see gRPC as a very powerful tool for synchronizing work between teams, which has historically been a hassle between us. By forcing us to define protobufs, we have a clear data structure that is going to be sent across our service. Better yet, with protoc, we will be sure that we are going to use the same structure in our code. This is arguably also possible with a REST API, using tools around the OpenAPI ecosystem. But we are too often constrained by time and we lack enough discipline and experience to identify that defining an API is often the most important part of the work, because a well-defined API means that we are not going into coding blind, that we have thought about caveats and edge cases, and more generally, architected our system. This might be looked at as a drawback to gRPC by some people, but in our case having this rigidity will be a blessing in a disguise for our team.

There are, however, also drawbacks of using gRPC in our case. I identified 3 of them:

* Our existing codebase has no gRPC code at all. This means we will need to rewrite all our code. This also might be a blessing in a disguise, since this is a way to eliminate the some of the technical debt that accumulated over time. As our first major project, we made a lot of mistakes in the way our code is structured, organized, and how it operates. This will also be a way for us to rethink about the mistakes we've made, reflect on them, and find ways to avoid them in our migration towards microservices. 
* As we migrate our whole system towards microservices, we will inevitably encounter problems. Whether it is conceptual misunderstanding of the underlying technologies or simple bugs in our code, there need to be strong debugging tools to help us find the problems. I would argue that gRPC is lacking in this field, compared to a REST API, where you can see in clear-text the payloads that are sent. This is useful for debugging and understanding what is transiting over the network, and to identify errors. This is due to gRPC being binary-encoded, and is a trade-off you have to do for the performance it provides.
* Lastly, during the migration process, we will be incentivized to copy-paste the old code. This is bound to happen, even if we clearly define it as a bad idea. This will lead to bringing bugs over, but more importantly, it will hinder the change of line of thoughts we need to have. REST APIs and gRPC Services are different concepts, and you don't build them in the same way. Since most of use has only used REST APIs as a way to communicate with other services, the transition is going to be difficult and we are going to be ending up with REST patterns in our gRPC services.

Overall, gRPC makes a lot of sense as our communication protocol and I would advise to use it as our sole synchronous inter-microservice communication protocol, mainly for its efficiency and rigidity, although the migration of the code base will be tedious for the team.

==== Istio ====
After some careful thinking, I've settled on using Istio at the operational layer. Let's discuss why.

Istio provides the biggest set of features out of all the service meshes solutions. Although we are probably not going to use them all, I think they provide interesting solutions for us to explore in term of load test in resiliency testing. We currently have very little users and it's hard for us to test the scalability and resiliency of our systems. Having artificial failures will help us better understand and identify bottlenecks in our system before they start to happen in real scenarios.

Using service mesh, and so Istio, we can improve the resiliency and fault-tolerance by easily implementing retries, circuit-breakers and timeouts. These patterns are usually requires a lot of technical, non business related code, when implemented at the application layer. We lift a big weight off our shoulder by not having to make a choice between time (in the form of coding) and unreliability.

Istio can be challenging to configure, but the main advantage is that its configuration is built in a declarative manner, using Kubernetes' CRDs. They can be scary to familiarize with, but the huge benefit for us is that it can be templated with Helm. We are using Helm to manage our deployments, and adding new templates to manage Istio resources into them is a matter of minutes. Configuring it is the hard part.

However, let's not forget the context of this project. This is an educational project, and setting some advanced challenges like this is hugely beneficial for us, if we manage to correctly understand what is going on behind the scenes and the concepts that revolves around Istio. Let's not forget that Istio is the most popular service mesh on the market today, and it is likely to stay this way for quite a while. Learning to work with it is a valuable skill, and will be useful for most of us in the future.

We are currently running on bare kubernetes service discovery, with no service mesh in-between. Although it is sufficient for what we are doing, the benefits and reasoning stated above is, for me, enough to justify its usage.

Since I've suggested using gRPC as our inter-microservice communication protocol, both of them blends in nicely with the proxyless features of Istio for gRPC services.

Unlike with gRPC, this suggestion is less about logical reasoning at a technical standpoint, but more about learning and gaining experience on a technology that we are likely to use in our professional careers.

==== Other suggestions ====
Unrelated to the technological suggestions above, we also need to focus on code quality, by delivering high-quality code, that is compatible with a microservice architecture. Code reviews are going to be important, and listening to everyone's suggestions and feedback will be more important than ever. We need to collaborate and push a environment where we foster upfront thinking about our architecture. We need to listen to everybody's worries and manage learning curves for each technology and the different burden it can have on some people.

==== Sequence diagram ====
To better illustrate how the flow of a request would be, I've made a sequence diagram highlighting all the hops a request will have to go through in our system. This is simplified and doesn't take into account how the proxies are configured, advanced configuration, and doesn't show the full scope of the request from a business logic standpoint. It focuses on the topic of this section, inter-microservice communication:

image::30_CommunicationBetweenMicroservices/30_Sequence_Diagram.png[]

We can see all our actors here:

* The gateway into our system, that accepts HTTP Request and forwards them with the appropriate gRPC service. This could be an application, but it also needs to have a foot in our application layer, since this is also the entry point into our service mesh. Istio allows to create gateways that handles quite a lot of technical logic, such as routing and rate limiting, and we should offload what we can to Istio. I would still have an application behind this though, that will be able to implement complicated business logic related concerns, such as fined-grained authentication and authorization.
* The proxies of our service mesh. These are the sidecars proxy that we create for each of our replicas. In Istio case, these are Envoy proxies. They take care of the communication layer for our microservices. All the traffic downstream is within our service mesh data plane, and encrypted. The upstream traffic goes to our microservices, unencrypted.
* Our microservices. These are our business logic units, that exposes gRPC services. To talk to other microservices, they use gRPC. However, they don't have to worry about service discovery, load balancing and general communication concerns, since the proxies are handling these concerns for them.

This sequence diagram shows pretty well the additional hops a request has to go through with a classic service mesh deployment. Each gRPC request/response goes through at least 3 hops when communicating from microservices to microservices: one from microservice A to its proxy, then from proxy A to proxy B, and finally from proxy B to microservice B. This is where the performance overhead of service meshes comes in, and why eBPF-based or proxyless service-mesh are relevant solutions when tight on resources, since they eliminates these additional hops while retaining most of the features.

=== Proof of concept ===
As a way to demonstrate how my technological suggestions would operate, I've made a simple proof of concept (PoC), that deploys 2 microservices and 1 gateway, all within an Istio service mesh in a Kubernetes cluster.

You can find the repo https://github.com/polycode-lucido/microservice-poc[here]. You will find further explanation in the readme, as well as how to use it and deploy it on your own.

== Conclusion ==
Inter-microservice communication is an essential part of designing your system. There are a lot of issues and concerns that arise from building a distributed system, that you don't find, or at least to a much lesser degree in a monolith application. We have identified these concerns, explained why they are important. Using protocols, technologies and patterns, we found ways to solve these problems, and settled on a few options that fulfills the requirements we have with Polycode. Inter-microservice communication is a critical aspect of an infrastructure and needs careful thinking, it is the backbone of your whole system.

Identifying problems and having ways to understand the state of your system goes hand-in-hand with this aspect. How do you know if your system is running as expected ?