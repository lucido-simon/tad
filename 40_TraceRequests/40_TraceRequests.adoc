= Tracing and logging =

== Introduction ==
Managing a distributed system like a microservices architecture can introduce new challenges, particularly when it comes to monitoring and debugging. When a request is made to a microservices system, it may involve multiple services working together to fulfill the request. This can make it difficult to understand exactly what happened to the request as it made its way through the system.

Monitoring a distributed system is key to understand where you system is weak. Without monitoring, you have no idea of the state of your system, where are the errors coming from, why some requests may fail, and what needs to be done to fix this.

There are three pillars that are essential for effective monitoring of a microservice system: traces, logs, and metrics. Those three pillars are also referred as telemetry.

* Traces provide a detailed view of the path of a request through the system, showing the sequence of events that occurred and how long each step took. They can also carry data added at each step, to have a further understanding of the request.
* Logs provide a record of what happened within each service, including any errors or issues that may have occurred. Logging is going to provide actual errors and this is usually where you can really understand what is going on in your system.
* Metrics provide a high-level overview of the system's performance and resource usage, helping you to identify trends and potential issues. Metrics can play a big role in scaling effectively your system (but you should also take into account business metrics, not only resource usage).

By combining these three pillars of monitoring, you can gain a comprehensive understanding of your microservices system and how it is functioning. This can be crucial for identifying and resolving issues, optimizing performance, and ensuring the overall reliability and stability of your system.

Let's dive deeper into two of those: traces and logs, and see how we can implement a good solution into our Polycode system.

== Tracing ==
Tracing is a technique that helps you understand the path of a request as it flows through a distributed system, such as a microservices architecture. It involves assigning a unique identifier to a request and propagating that identifier throughout the system as the request is processed. This allows you to see the complete lifecycle of a request and understand how it flowed through the system.

It helps tracking Service Level Indicators (SLI) of your system, with the helps of metrics, depending on the specific SLI. A SLI represents an information about your system, such as the time your page takes to load. SLIs are often linked to Service Level Objectives (SLO), which are a targeted objective that a service tries to provide. This all links to Service Level Agreements (SLA), which defines the standard a customer can expect from a provider. We might guarantee 99.9% uptime to our customers, that is our SLA, with a SLO that is 99.99% uptime for critical microservices, and we track this data using SLIs.

[#40Terminology]
The terminology I will use is the following:

* Span is a unit of work within a trace. It typically corresponds to a single service operation or network request. Spans are typically identified by a unique span ID. Spans can also include metadata, such as timestamps and tags, which provide additional information about the span and how it fits into the overall trace. A new span is usually created for every microservice the request hop through.
* Trace is a series of spans that represent the path of a request through a distributed system. Each span in the trace is connected to the one before it by a parent-child relationship, which shows how the request flowed through the system. Traces are typically identified by a unique trace ID.
* Baggage is a term used in distributed tracing to refer to a piece of data that is carried along with a trace or span as it flows through a system. Baggage is often used to include contextual information about a trace or span that may be useful for debugging or analysis. For example, you might use baggage to include information about the user who made a request, the environment the request was made in, or the application version that was in use when the request was made.

Tracing can be a powerful tool for finding and debugging problems in a microservices system. By being able to see the complete lifecycle of a request and how it flowed through the system, you can identify bottlenecks, errors, and other issues that may be affecting the performance or reliability of your system. Tracing can also help you understand the impact of changes to your system, such as when you deploy a new version of a service or make changes to the system's architecture.

== Logging ==
Logging is the process of recording events and messages that occur within a system. In a microservices architecture, logging is an important tool for understanding what is happening within each service and how the services are interacting with each other. By capturing log messages at various points in the system, you can gain insight into the behavior and performance of your system and identify any issues or problems that may be occurring.

There are a few key concepts to understand when it comes to logging in a microservices system:

* Log message: A log message is a record of an event or message that occurs within a system. Log messages typically include a timestamp, a severity level, and a message string that describes the event.
* Log level: A log level is a way to classify log messages based on their importance or severity. Commonly used log levels are "debug", "info", "warning", and "error".
* Log sink: A log sink is a destination for log messages. Log sinks can be local files, databases, cloud storage, or any other location where log messages can be stored and accessed.
* Log aggregation: Log aggregation is the process of collecting log messages from multiple sources and storing them in a central location. This can be useful for making it easier to search and analyze log messages from across the system.

By carefully designing and implementing your logging strategy, you can use log messages to understand what is happening within your microservices system and identify issues and problems as they occur. This can be crucial for ensuring the reliability and stability of your system and for quickly identifying and resolving any issues that may arise.

== OpenTelemetry ==
During this section, we will focus on solutions based on OpenTelemetry (which I will refer as OTEL). OpenTelemetry is an open source observability framework created to help developers instrument, generate, collect, and export telemetry data (such as metrics, traces, and logs) for their applications. It aims to provide a vendor-neutral and consistent way of generating, collecting, and exporting telemetry data across a variety of programming languages, platforms, and clouds.

OpenTelemetry was created through the merger of two open source projects: OpenCensus and OpenTracing. OpenCensus was initially developed by Google in 2016 as a way to standardize the collection of telemetry data across Google's services. OpenTracing, on the other hand, was developed by a group of companies in 2015 as a specification for distributed tracing of applications.

In May 2019, the OpenCensus and OpenTracing communities decided to merge their efforts and create a new project called OpenTelemetry. The goal of the merger was to combine the strengths of both projects and provide a single, unified observability framework that could be used by developers across a variety of platforms and languages.

Since its creation, OpenTelemetry has gained widespread adoption and is now supported by many cloud providers and open source projects. It has also become the de facto standard for observability in cloud-native applications.

You will most likely encounter proprietary solutions in the wild, but this tends to phase out to the eagerly-adopted OTEL solution. OpenTelemetry offers a standardized protocol for metrics, traces but also logs to collect data and centralize them, and the industry has recognized the power of having an open-source standard. More and more solutions (even proprietary one) are moving to using this standard. 
Before OpenTelemetry, there were a number of different tools and approaches for collecting and exporting telemetry data from applications. Many companies and organizations developed their own proprietary solutions for this purpose, and there was no widely accepted standard for how to do it.

Since OpenTelemetry looks to be dominating the market and is pushing to be the one and only standard new applications and system are willing to use, I suggest to use it for all new applications that requires tracing, logging and metrics, such as Polycode. This is why I will now focus on OpenTelemetry as the de-facto protocol we are going to use.

=== Vocabulary ===
Let's define some key vocabulary revolving around OpenTelemetry. This is not a exhaustive list and should be seen as a completion of the xref:40Terminology[vocabulary previously defined].

* Instrumentation: The process of adding code to an application to collect telemetry data. In OpenTelemetry, instrumentation is typically done using SDKs or libraries provided by the project.
* Exporters: Components that receive telemetry data from an application and send it to a specific destination, such as a monitoring service or a log aggregator.
* Collection: The process of gathering telemetry data from an application and sending it to a destination for storage or analysis.
* Sampling: The process of selecting a subset of telemetry data to be collected and exported. Sampling can be used to reduce the volume of data being collected and exported, and can be based on various criteria such as the rate of data being generated or the importance of the data.
* Context propagation: The process of carrying telemetry data, such as trace and span information, between different components or services in a distributed system. Context propagation is often used to maintain a consistent view of telemetry data as it moves through a system.
* Correlation: The process of linking together telemetry data from different sources or at different points in time to understand the relationships between them. In OpenTelemetry, this is often done using trace and span IDs.
* Tagging: The process of adding metadata to telemetry data in the form of key-value pairs. Tagging can be used to add context or additional information to telemetry data, and can be used to filter or group data when analyzing it. This can also be seen as baggages.
* Exporters: Components that receive telemetry data from an application and send it to a specific destination, such as a monitoring service or a log aggregator. OpenTelemetry provides a variety of exporters for different platforms and destinations.
* Tracing backend: A service or system that receives trace data from an application and stores it for analysis and visualization. OpenTelemetry supports a variety of tracing backends, including open source and commercial options.

=== Logging with OpenTelemetry ===
Historically, traces and logs were often treated as separate types of data and had different tools and systems for collecting, storing, and analyzing them. This could make it difficult to understand the relationships between trace data and log data, and to use them together to understand the behavior and performance of a system.

Making logs context and request-scoped would make them much more valuable and would allow for a much easier time to understand where your bottlenecks and errors comes from. This is why OTEL has been pushing solutions to include logs with traces and metrics to better understand and analyze what's going on in your system. We end up with correlated telemetry, which is enriched with data coming from the three pillars of monitoring.

=== Overview of a typical OpenTelemetry system ===
Here's a simplified overview of how a OTEL system typically works:

image::40_TraceRequests/40_OTEL_Overview.png[]

Each of your application usually integrates an OpenTelemetry SDK, allowing to easily integrate the protocol and the necessary processing within your application. For each requests, your application should notify your SDK of a new span, and take the necessary action. Your SDK will emit data to an OTEL Collector, which is a single centralized endpoints for all your application to communicate with. This collector will do some processing, and export its data to the backend of your choice. Once in here, you can access your User Interface and check your traces and logs.

As mentioned above, this is heavily simplified, and depending on the tools and configuration you choose, you can have some of these components coming in a single package. This diagram shows where and how you can choose multiple solutions, which can all work together thanks to the standardized OpenTelemetry Protocol. Usually, your OpenTelemetry SDKs within your application are totally unaware of the collector you're using, except that it understands the OpenTelemetry Protocol.

Your collector can also support other tracing backend, such as the Jaeger or Zipkin protocol for example. With this added layer, you can customize and choose how to handle your data before storing them into your backend. It is common to have OTEL tools that comes with your collector, your backend and your user interface in a single package. It is usually possible to customize and replace each of these bricks in such systems.

== Tools ==
=== Visualization tools ===
One of the important aspect when monitoring the system, is not only to collect valuable information, but also have great tools to visualize and search this data. In this section we will look at the most complete solutions available on the market.

==== The ELK Stack ====
The Elastic Stack (formerly known as the ELK Stack) is a set of open source tools for collecting, storing, and analyzing logs and other types of data. It is developed by Elastic, a company that provides a range of products and services for search, analytics, and observability.

The Elastic Stack consists of the following components:

* Elasticsearch: A distributed, RESTful search and analytics engine that is used for storing and indexing logs and other types of data.
* Logstash: A data processing pipeline that can be used to collect, parse, and transform logs and other types of data before storing it in Elasticsearch. Logstash is highly configurable and can be used to process a wide variety of data sources.
* Kibana: A web-based visualization tool that can be used to analyze and visualize data stored in Elasticsearch. Kibana provides a range of features for searching, filtering, and visualizing data, and can be used to create dashboards and alerts for monitoring and alerting purposes.

The Elastic Stack is widely used for collecting, storing, and analyzing logs and other types of data. It is particularly popular in environments where there is a large volume of data being generated, and is often used in combination with other observability tools such as APM (Application Performance Management) solutions and tracing systems.

In terms of traces, the Elastic Stack can be used to store and visualize trace data in a number of ways. For example, trace data can be collected using an OpenTelemetry exporter and sent to Elasticsearch for storage. Kibana can then be used to visualize and analyze the trace data, using features such as filtering, aggregation, and graphing. The Elastic Stack can also be used in combination with other tracing tools, such as Jaeger or Zipkin, to provide a more comprehensive view of trace data.

==== Jaeger ====
Jaeger is an open source distributed tracing system developed by the Cloud Native Computing Foundation (CNCF). It is designed to help developers understand the behavior and performance of distributed systems by providing a way to collect and visualize trace data.

In the context of OpenTelemetry, Jaeger can be used as a tracing backend to store and analyze trace data generated by OpenTelemetry-instrumented applications. OpenTelemetry provides a Jaeger exporter that can be used to send trace data from an application to a Jaeger server for storage and analysis.

Jaeger provides a number of features for visualizing and analyzing trace data, including a user-friendly web interface for searching and filtering traces, and a variety of different visualization options for understanding trace data. It also has integrations with a variety of other observability tools, such as Prometheus and Grafana, which can be used to extend its capabilities or to integrate trace data with other types of data.

Jaeger uses a distributed architecture, with a collector component that receives trace data from clients and stores it in a storage backend, and a query component that provides a web interface for searching and visualizing trace data. The collector and query components can be deployed separately, allowing for flexibility in terms of scale and performance.

It natively supports two NoSQL databases as trace storage backend : Cassandra, and Elasticsearch. This means you can integrate Jaeger with your ELK stack. It also has backward compatibility with Zipkin (which has historically been the go-to tracing system, but is being more and more replaced by OTEL). It is not relevant to us, in our Polycode application, but it is important to note as it might be something you're looking for.

== Polycode ==
I've decided to use the ELK stack for the Polycode application. This is because it is a complete solution, that allows for collecting logs, metrics and traces and correlating them together. This is however, a big stack and will consume resources, with the added benefits of being highly scalable. Traces, metrics and logs can easily be exported using adapted OTEL exporters in the code. The elastic ecosystem provides a multitude of tools to handle traces metrics and logs their way, but I've decided to use OTLP for exporting data out of my application. This way, I've not closed the door for other monitoring system if we decided that this stack wasn't suited for us or if we identified some problems with it. We could very easily plug a Jaeger collector instead of the APM Server and have our monitoring stack working again.
APM has been supporting the OpenTelemetry Protocol for a while now, adding supports for the more recent logging protocol in version 8 of APM. Collecting logs this way eliminates the need to have an additional pipeline for logs. If we were to use the recommended elastic's log collection process, we would need to spin up a Filebeats for every application, sending their data to Logstash.
The main advantage of using this process is that you can also easily export kernel logs from each of the container, but I don't think it is relevant and worth the hassle in Polycode.

=== Deployment ===
One important aspect to keep in mind, is that your monitoring system will be at the heart of debugging and discovering problems as soon as possible in your application. This means you need to have it up and running even if you have a major outage in your primary system. This is why I think your whole stack should be separated in an external system. This also ensure that your monitoring and operational system does not impact your application performance.

This is the deployment diagram:

image::40_TraceRequests/40_Deployment_Diagram.png[]

As you can see, the whole monitoring stack is totally separated from the main application cluster, to make sure not to impact it, and to retain observability in the case of a major system failure. I've decided to run the ELK stack on only one ElasticSearch node, mainly to not over-complicate the already complicated architecture. It is certainly regrettable if we were to lose our data, as we would lose the ability to correlate trends in our system over time, but this is data we can rebuild over time.

=== Sequence diagram ===
To better understand the flow of the system, I've decided to show you a sequence diagram of the action of accepting a team invite by a user. Please bare in mind that I've not shown any error case in this sequence diagram, but any error should be also traced and sent to the APM Server. Here's the diagram:

image::40_TraceRequests/40_Sequence_Diagram.png[]

We find all our application components that are actors in this requests:

* The application frontend, which will take users action and send appropriate requests to the gateway.
* The API gateway, which is responsible for taking the HTTP Request of the frontend, authenticating the user if needed via its authentication cookie, and redirecting the request to the correct microservice as a gRPC request.
* The User microservice (practice context, not account context), which is our transactional boundaries for all operations done on an user in the practice context. To mutate a user and a team state, we must go through this aggregate. If it is not clear, or if you don't recall why, I suggest you refer to the first part of this paper and we go in details in the hows and whys of this architecture.
* The team microservice, which is responsible for handling the business logic of our team in our system.
* The APM Server, which in our stack corresponds to the OTEL Collector, and is in direct communication with elasticsearch to process and store the traces.

All communications with the APM Server is done with the OpenTelemetry Protocol (OTLP).

Once the requests reaches our backend, and more specifically our API Gateway, a new trace is created. The gateway creates its span, which will be the root span of the request, and since it is able to resolve the user with its authentication cookie, it will include its information as baggage in our trace. It then forwards the request to the user microservice, which will create a new span within the trace. Each hop on a microservice will create a new span. This is important to understand the flow of the request through the system. This principle trickles down to the Team microservice, with each corresponding spans being closed when the response is sent back.

As mentioned above, if an error where to occur, it is important to bubble up the error to our trace, and a correct implementation of our tracing system in each of our microservice is crucial to achieve this goal. You usually want to consult traces when you have errors on your system, and if you can't find them or if they are incomplete, you are probably not using your SDKs properly.

If any logs were to be printed during this process and within the scope of this trace, the Trace ID and the Span ID would be included and sent to the OTEL collector to its log endpoint. This way, we can easily correlates logs with traces.

Not shown in this diagram is the subsequent processing of the request, how it is stored, indexed, filtered, etc.. This is specific to each implementation, and configuration of your solution. It doesn't matter here, since the overall sequence would be the same. In fact, you can replace our APM Server by any OTLP-Compliant collector and you will still have a working system.

If a system administrator wants to look at details of this request, it can log into Kibana, search for this specific request with appropriate filters, and see how long each spans took, the logs related to it, which user initiated the request, the eventual errors and other related information.

== Conclusion ==
Observability is a key component in a distributed system, it is a crucial component to have insights about what is happening in your system. The more complex your system grow, the more you will be in the dark about how it behaves without implementing the necessary observability toolkit. I've decided to go towards the OTEL Standard in Polycode, for its standardized instrumentation, wide range of library, and for being open source and vendor neutral, despite the increased complexity of the system and its relatively recent ecosystem, although its production ready and pretty mature. As actual tools and implementation, I've decided to go with the ELK stack (without Logstash, since we're using OTLP log support for logs), for its matureness, robustness, scalability, despite the paid features and the heaviness of the stack.


