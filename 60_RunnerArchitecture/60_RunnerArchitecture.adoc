= Runner architecture =

== Introduction ==
Throughout this paper, I've glossed over a very important part of Polycode: the runners. I've told you multiple times that I wasn't diving in deeper the subject because there was going to be a section dedicated to it; here we are. In this section, I'm going to talk about what are runners, what are the constraints we need to respect, how can we create a system that scales in a secure and efficient manner and we will finish, as always, by taking a look at some diagrams to better understand the decisions I've made for Polycode.

Runners are the backbone of our application. This is what drives the interactivity and the engagement, since it is at the core of every piece of code that is executed on the website. For the user, it looks seamless, but there is a great deal of complexity and thoughts that must be put into it.

=== What are runners ===
We need to define what runners are and what they aren't. As you might have figured, runner run code. More specifically, they run the code fed by the user when they resolve an exercise. The code the user wrote is then being fed by the input of each validators that might exist for the exercise, and the standard output and standard error streams are returned.

There are a few key points that needs to be made clear. First off, runners are more or less agnostic from the underlying (or overlying, depending on how you see it) application. It does not now what validators are, or what contents are, they just take pieces of code and run it against some input. It could very well be used for some online Rust playground with some very minor tweaks. Although we must be conscious about the Polycode's requirements and constraints, we are far enough from the core business logic that most of the decisions will be made on a technical and human standpoint.

Furthermore, since the runners execute user input that can't be trusted, since anybody can use our platform, we need some strong security and isolation mechanism in place to make sure that:

* A user can't disrupt the stability of the platform
* A bad actor can't escape the isolation and access the system

As of right now, we need our runners to be able to run code written in Javascript, Rust, Java and Python, with project composed of one or multiple files. This means that our runner architecture needs to be able to compile Rust code, and our actual runtime needs to have the appropriate tools and interpreters to execute the code that is being given to them.

Program input is a string, that can include newlines. This is a pretty simple input system, and there is no way to influence the input based on the output of the running program. This means validators must be deterministic.

== Running code in an isolated manner ==
As I've mentioned in the introduction, one of the key point and constraint we need to respect is to have the user code run in a totally isolated manner. It should not be able to interact with the host system. A great way to think about runners is a serverless platform that you can find in most public cloud nowadays, like AWS's lambda. You give AWS a piece of code, that will be executed somewhere in their cloud. This is the same principle that we have with Polycode: the user give us a piece of code and they want it executed somewhere in our system. Just like AWS and its customers, we don't want the code to be able to escape the boundaries we have given them.
In this chapter, we will look at different ways we can achieve this goal, the pros and cons of each of them and what it implies.

=== Containers ===
The most basic and easy way to isolate programs, is to run them within a container. A container is nothing more than a process running on the host system, that has been configured in a way to typically run in its own virtual filesystem, with limited network connectivity, no knowledge of other running programs, limited in resources and isolated from the host machine. They allows running multiple isolated systems on a single host, even if those system tries to access the same resources, the same port for example.

This looks like everything we need, with no major drawback ! Right ? Not so easy unfortunately.
The isolation is done at the operating system level, opening a great and somewhat high level surface of attack. Escaping from container isolation is something that can be done pretty easily, if the container configuration is faulty. This can be especially dangerous if the container is running as the root user, as the attacker would then have full access to the host system.
Just like with any software, containers can contain vulnerabilities that need to be patched. If these patches are not applied in a timely manner, the container could be at risk. This is not a huge deal in our case, since the container lifespan is the one of the program of the user, which is pretty short. Moreover, the user could only compromise its own container, as long as they are not able to escape it.

Because containers share the host's kernel, a containerized application that is able to compromise the kernel could potentially gain access to other containers on the same host, as well as the host itself. This could allow an attacker to gain access to sensitive data or to compromise other systems on the network. This is exactly what we want to avoid, and it looks like running unsafe code in a container will not give us sufficient isolation to bring the risk down to comfortable levels.

Moreover, managing resource of a container can be tricky. The main and only available solution out there is to kill a container that is grabbing more memory or CPU resources than it should. Ideally, we would want a system that is actually able to cap the usage of a program, making sure that it can't grab more resources than it should in the first place.

One last concern to have about containers in our system, is that we are probably going to run them inside an already dockerized environment. This means we need to run our runtime in a Docker in Docker (DinD) setup. DinD can introduce additional security risks. Because the Docker daemon inside the container has access to the host system's Docker daemon, any vulnerabilities in the container could potentially be exploited to compromise the host system. This is especially concerning if the container is running with elevated privileges, such as the root user.
Another drawback is that DinD can have performance overhead compared to running the Docker daemon directly on the host system. Because the Docker daemon inside the container needs to communicate with the Docker daemon on the host system, there is additional overhead in the form of network communication and process management. This results in reduced resource efficiency.

=== Virtual Machines ===
Another big tool for creating isolated environments at our disposal is virtual machines (VMs). Today, the world runs on VMs. Every cloud providers, private or public, relies heavily on VMs to divide a bare-metal server resources into smaller, individual and isolated machines.

VMs are using modern processors virtualization capabilities, this means our workload are isolated thanks to hardware implementation, whatever the host and guests operating systems. This allows for a much tighter isolation, and security. There is a big notion in the previous statement: the guest runs its own operating system. This means that there is no collision at the kernel level that can occurs, since we would spin up a virtual machine for each of the user's request. It can compromise the whole guest system, and still be isolated, thanks once again to hardware virtualization. Escaping from hardware virtualization is significantly harder, at level that I think are comfortable.

VMs also allows us to better isolate resources, since we can configure the available resources in each VMs. Unlike with containers, the guest operating system will be responsible for managing those resources, and we have a clear boundary on them. It can't use more than what we gave it. However, operating systems typically have system in place to make sure to continue functioning properly even in the case where it needs resources it doesn't have. For example, if you need more RAM, your operating system usually have swap pages that it can use as if it was RAM, at a very significant performance cost.

This also means that we need that the overall resources usage will go up, since every code execution also means spinning up an operating system. However, Linux has some great option, providing kernels that not only takes fraction of seconds before becoming operational, but also very little resources. Linux images can be customized to our needs, tuning them to our needs, to further decrease resource usage and boot time. We can also imagine having special VMs that run special operating systems, such as Windows. This would allow for executing code that is targeted at win32 for example. This also opens up the possibility of running and executing code compiled for different processors architecture. Everybody loves assembly, right ? This would allow to create content on RISC-V assembly for example, even if those VMs would be extremely slow, since we are not virtualizing anymore, but emulating. It also means we need to use a virtual machine manager capable of emulating such system. But we open a native experience for the user.

However, there is also a increased complexity with VMs: talking to them is not as easy as with processes. With containers, you could programmatically run commands easily, with solutions like Docker for example. With VMs, you either need to have a custom kernel that is built to run some commands at startup, and find a way to extract executions results out of them, or you need to find a way to communicate with an agent in the VM (still meaning a custom linux image), that will respond to commands from the host.

All of this complexity might be a blessing in disguise: this opens up a world of possibilities in how we run our runners. Having an agent system that takes in request allows for a lot of features and optimization: preemptive startup (starting VMs before actually needing them, with them waiting for requests), fine-tuning execution parameters on the fly, or even executing custom operations depending on the code being executed, that can't be known ahead of time.

Overall, virtual machines gives us a satisfactory level of isolation, at the cost of increased resource usage and complexity. However, I would argue that the tradeoff is worth it, and it goes beyond that: we must accept every challenges to make sure we have good isolation and a secure system.

=== Serverless ===
As I've eluded earlier, what we are trying to accomplish is very similar to what cloud providers have been doing for years now: taking some small pieces of code, execute it and exit. Why not base ourselves on the work these companies has already done, instead of reinventing the wheel ? Let's look at the pros and cons of using serverless services cloud providers offer.

First, let's define what serverless actually is. Serverless is a cloud computing execution model in which the cloud provider dynamically manages the allocation of resources and charges for the execution of code. With serverless, you can run your code without having to worry about the underlying infrastructure, such as servers or virtual machines.

In a serverless model, you write and deploy your code in the form of functions, and the cloud provider executes the code in response to triggers, such as HTTP requests or events. The cloud provider automatically allocates the necessary resources to execute the code, and you are only charged for the actual execution time and the number of requests or events processed.

This fits particularly well what we are trying to achieve. However, there is an additional variable in all of this: cost. Indeed, until now, we didn't worry about what the cost of our solutions were, since we are currently using machines that are at our disposal. With these serverless services, you have to pay for the execution time of your code.

Another thing to demystified, is that it's not "send the code of the user to the serverless service and everything happens magically". We need to write a piece of code that takes a user input as a requests, figure out the inputs the program should be running, then start a runtime environment to run that code, while compiling any code before running, if applicable. We also need to need to properly isolate the code, and manage permissions of our serverless application properly, to make sure that the user doesn't have access to any other resources that is at disposal in our cloud platform account.

Serverless platform usually runs in a pretty trimmed-down system, and you have not a lot of guarantees and possibilities about the system that you're running on. This means you might not have the needed dependencies. Serverless was made for serving HTTP requests, opening some sockets to some databases or other HTTP connection along the way. Not for spawning another program. Serverless is more complex to design and implement than traditional architectures, because you need to consider the specific constraints and limitations of the serverless model.
In addition, the additional abstraction introduced by the serverless model can make it more difficult to debug and troubleshoot issues, because you don't have direct access to the underlying infrastructure.

If we forget the fact that we have a base infrastructure already at our disposal, and that we don't currently have to pay for anything, cost become an interesting factor that might justify putting the work into making serverless work.

With serverless, you pay for execution time. This means that if you have a low volume of user using your service, you might need to execute code during 1 hour in the day. With serverless, you would pay the rate for code execution of 1 hour. If you have a server is idling, waiting to spawn VMs, this server will run during 24 hours, and you will be charged for this period. Even if you can get much more with your money with VMs, if you don't have the usage to justify it, it is not cost-effective, and you might rather take the decision to use serverless services. Moreover, serverless is scalable basically infinitely, since you rely on your cloud providers resources. This comes with other benefits cloud providers typically gives you, such as a near-always uptime, hardware, hypervisors and operating system maintenance for this kind of highly-abstracted services, flexibility, global availability, etc..

The right balance, firmly on a cost of operation effectiveness standpoint is most likely somewhere in the middle. We will talk about it more later in this section.